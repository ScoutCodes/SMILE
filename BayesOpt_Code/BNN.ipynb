{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Model for Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the true objective function z = f(x, y)\n",
    "def obj_func(x, y):\n",
    "    return -(20 * (1 - np.exp(-0.2 * np.sqrt(0.5 * (x**2 + y**2)))) - np.exp(0.5 * (np.cos(2 * np.pi * x) + np.cos(2 * np. pi * y))) + np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Noise that will be added to data Response\n",
    "ran_err = lambda n, x: np.random.normal(0, x, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for training the neural network\n",
    "def generate_data():\n",
    "    x = np.random.uniform(-35, 35, 10)\n",
    "    y = np.random.uniform(-35, 35, 10)\n",
    "    z = obj_func(x, y)\n",
    "    return np.vstack((x, y)).T, z\n",
    "\n",
    "# Generate the data\n",
    "X, z = generate_data()\n",
    "z = z + ran_err(z.shape, 0.05)\n",
    "\n",
    "\n",
    "# Preprocess the data \n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "hyp_space  = [\n",
    "    Integer(1, 5, name='num_layers'), # Num of layers in the network (depth)\n",
    "    Integer(10, 100, name='num_units'), # Num of neurons in each hidden layer (width)\n",
    "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'), # Steps size at each iteration \n",
    "    Real(0.0, 0.5, name='dropout_rate'), # Probability of droping out a neuron\n",
    "    Integer(10, 100, name='batch_size'), # Num of samples per batch\n",
    "    Integer(1, 20, name='epochs') # Num of epochs (iterations over the entire dataset) during training\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x0000024988FDA520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x000002498D378680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 40 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000024988FED760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 70 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000002499F2EEB60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Best hyperparameters:\n",
      "num_layers: 2\n",
      "num_units: 88\n",
      "learning_rate: 0.002635264040018728\n",
      "dropout_rate: 0.4004553759898222\n",
      "batch_size: 57\n",
      "epochs: 14\n"
     ]
    }
   ],
   "source": [
    "# Defining the neural network model\n",
    "# def NN_model(num_layers, num_units, learning_rate, dropout_rate):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(num_units, activation='relu', input_shape=(2,)))\n",
    "#     for _ in range(num_layers - 1):\n",
    "#         model.add(Dense(num_units, activation='relu'))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "#     return model\n",
    "\n",
    "def NN_model(num_layers, num_units, learning_rate, dropout_rate):\n",
    "    inputs = Input(shape=(2,))\n",
    "    x = Dense(num_units, activation='relu')(inputs)\n",
    "    for _ in range(num_layers - 1):\n",
    "        x = Dense(num_units, activation='relu')(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# # Define the objective function to minimize\n",
    "# @use_named_args(hyp_space)\n",
    "# def objective(**params):\n",
    "#     model = KerasRegressor(model=NN_model, **params, verbose=0)\n",
    "#     return -np.mean(cross_val_score(model, X, z, cv=3, n_jobs=-1, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# Define the objective function to minimize\n",
    "@use_named_args(hyp_space)\n",
    "def objective(**params):\n",
    "    num_layers = params['num_layers']\n",
    "    num_units = params['num_units']\n",
    "    learning_rate = params['learning_rate']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    model = NN_model(num_layers, num_units, learning_rate, dropout_rate)\n",
    "\n",
    "    kfold = KFold(n_splits=3)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(X):\n",
    "        model.fit(X[train_idx], z[train_idx], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        score = model.evaluate(X[val_idx], z[val_idx], verbose=0)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "result = gp_minimize(objective, hyp_space, n_calls=50, random_state=0, acq_func='EI')\n",
    "\n",
    "# Output best hyperparameters from BayesOpt\n",
    "print(\"Best hyperparameters:\")\n",
    "print(\"num_layers:\", result.x[0])\n",
    "print(\"num_units:\", result.x[1])\n",
    "print(\"learning_rate:\", result.x[2])\n",
    "print(\"dropout_rate:\", result.x[3])\n",
    "print(\"batch_size:\", result.x[4])\n",
    "print(\"epochs:\", result.x[5])\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = NN_model(\n",
    "    num_layers=result.x[0],\n",
    "    num_units=result.x[1],\n",
    "    learning_rate=result.x[2],\n",
    "    dropout_rate=result.x[3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 745ms/step - loss: 405.0016\n",
      "Epoch 2/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 323.3695\n",
      "Epoch 3/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 249.6107\n",
      "Epoch 4/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 230.5619\n",
      "Epoch 5/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 192.7144\n",
      "Epoch 6/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 137.8011\n",
      "Epoch 7/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 102.0413\n",
      "Epoch 8/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 91.2842\n",
      "Epoch 9/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 83.1355\n",
      "Epoch 10/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 71.5370\n",
      "Epoch 11/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 69.6246\n",
      "Epoch 12/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 61.7392\n",
      "Epoch 13/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 84.4518\n",
      "Epoch 14/14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 61.5925\n",
      "New points to sample:\n",
      "[[  9.63692505  -3.01086525]\n",
      " [  2.40671578  13.46465717]\n",
      " [-25.16210108 -13.28217739]\n",
      " [-27.61513721   7.00765855]\n",
      " [ 28.43192733  26.54706153]\n",
      " [ 31.98836191  -2.01453789]\n",
      " [  6.19355122  28.8797503 ]\n",
      " [  8.11582837  32.12381552]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "best_model.fit(X, z, epochs=result.x[5], batch_size=result.x[4], verbose=1)\n",
    "\n",
    "# Function for MC Dropout predictions\n",
    "def mc_dropout_predictions(model, X, num_samples=100):\n",
    "    predictions = np.zeros((num_samples, X.shape[0]))\n",
    "    for i in range(num_samples):\n",
    "        predictions[i, :] = model(X, training=True).numpy().flatten()\n",
    "    prediction_mean = predictions.mean(axis=0)\n",
    "    prediction_std = predictions.std(axis=0)\n",
    "    return prediction_mean, prediction_std\n",
    "\n",
    "# Make predictions with MC Dropout\n",
    "mean, std = mc_dropout_predictions(best_model, X)\n",
    "\n",
    "# Selecting new points with highest uncertainty\n",
    "num_new_points = 8  # Num of new points to sample\n",
    "new_points_indices = np.argsort(std)[-num_new_points:]\n",
    "new_points = X[new_points_indices]\n",
    "\n",
    "print(\"New points to sample:\")\n",
    "print(new_points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
