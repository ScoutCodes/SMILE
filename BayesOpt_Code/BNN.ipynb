{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Model for Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the true objective function z = f(x, y)\n",
    "def obj_func(x, y):\n",
    "    return -(20 * (1 - np.exp(-0.2 * np.sqrt(0.5 * (x**2 + y**2)))) - np.exp(0.5 * (np.cos(2 * np.pi * x) + np.cos(2 * np. pi * y))) + np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Noise that will be added to data Response\n",
    "ran_err = lambda n, x: np.random.normal(0, x, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for training the neural network\n",
    "def generate_data():\n",
    "    x = np.random.uniform(-35, 35, 10)\n",
    "    y = np.random.uniform(-35, 35, 10)\n",
    "    z = obj_func(x, y)\n",
    "    return np.vstack((x, y)).T, z\n",
    "\n",
    "# Generate the data\n",
    "X, z = generate_data()\n",
    "z = z + ran_err(z.shape, 0.05)\n",
    "\n",
    "\n",
    "# Preprocess the data \n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "hyp_space  = [\n",
    "    Integer(1, 5, name='num_layers'), # Num of layers in the network (depth)\n",
    "    Integer(10, 100, name='num_units'), # Num of neurons in each hidden layer (width)\n",
    "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'), # Steps size at each iteration \n",
    "    Real(0.0, 0.5, name='dropout_rate'), # Probability of droping out a neuron\n",
    "    Integer(10, 100, name='batch_size'), # Num of samples per batch\n",
    "    Integer(1, 20, name='epochs') # Num of epochs (iterations over the entire dataset) during training\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x000001471CC9A5C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x0000014721038680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 70 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000147382BF9C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Best hyperparameters:\n",
      "num_layers: 4\n",
      "num_units: 100\n",
      "learning_rate: 0.004092972872628451\n",
      "dropout_rate: 0.0\n",
      "batch_size: 100\n",
      "epochs: 12\n"
     ]
    }
   ],
   "source": [
    "# Defining the neural network model\n",
    "# def NN_model(num_layers, num_units, learning_rate, dropout_rate):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(num_units, activation='relu', input_shape=(2,)))\n",
    "#     for _ in range(num_layers - 1):\n",
    "#         model.add(Dense(num_units, activation='relu'))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "#     return model\n",
    "\n",
    "def NN_model(num_layers, num_units, learning_rate, dropout_rate):\n",
    "    inputs = Input(shape=(2,))\n",
    "    x = Dense(num_units, activation='relu')(inputs)\n",
    "    for _ in range(num_layers - 1):\n",
    "        x = Dense(num_units, activation='relu')(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# # Define the objective function to minimize\n",
    "# @use_named_args(hyp_space)\n",
    "# def objective(**params):\n",
    "#     model = KerasRegressor(model=NN_model, **params, verbose=0)\n",
    "#     return -np.mean(cross_val_score(model, X, z, cv=3, n_jobs=-1, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# Define the objective function to minimize\n",
    "@use_named_args(hyp_space)\n",
    "def objective(**params):\n",
    "    num_layers = params['num_layers']\n",
    "    num_units = params['num_units']\n",
    "    learning_rate = params['learning_rate']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    model = NN_model(num_layers, num_units, learning_rate, dropout_rate)\n",
    "\n",
    "    kfold = KFold(n_splits=3)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(X):\n",
    "        model.fit(X[train_idx], z[train_idx], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        score = model.evaluate(X[val_idx], z[val_idx], verbose=0)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "result = gp_minimize(objective, hyp_space, n_calls=50, random_state=0, acq_func='EI')\n",
    "\n",
    "# Output best hyperparameters from BayesOpt\n",
    "print(\"Best hyperparameters:\")\n",
    "print(\"num_layers:\", result.x[0])\n",
    "print(\"num_units:\", result.x[1])\n",
    "print(\"learning_rate:\", result.x[2])\n",
    "print(\"dropout_rate:\", result.x[3])\n",
    "print(\"batch_size:\", result.x[4])\n",
    "print(\"epochs:\", result.x[5])\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = NN_model(\n",
    "    num_layers=result.x[0],\n",
    "    num_units=result.x[1],\n",
    "    learning_rate=result.x[2],\n",
    "    dropout_rate=result.x[3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 384.1572\n",
      "Epoch 2/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 215.6589\n",
      "Epoch 3/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 79.6818\n",
      "Epoch 4/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 35.4972\n",
      "Epoch 5/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 96.9888\n",
      "Epoch 6/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 55.8725\n",
      "Epoch 7/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 13.0929\n",
      "Epoch 8/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.2735\n",
      "Epoch 9/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 18.1489\n",
      "Epoch 10/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 26.8234\n",
      "Epoch 11/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 27.3492\n",
      "Epoch 12/12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 21.8742\n",
      "New points to sample:\n",
      "[[  6.92105965  16.27768281]\n",
      " [-29.86363251 -13.1036587 ]\n",
      " [ 17.98167361  12.66344814]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "best_model.fit(X, z, epochs=result.x[5], batch_size=result.x[4], verbose=1)\n",
    "\n",
    "# Function for MC Dropout predictions\n",
    "def mc_dropout_predictions(model, X, num_samples=100):\n",
    "    predictions = np.zeros((num_samples, X.shape[0]))\n",
    "    for i in range(num_samples):\n",
    "        predictions[i, :] = model(X, training=True).numpy().flatten()\n",
    "    prediction_mean = predictions.mean(axis=0)\n",
    "    prediction_std = predictions.std(axis=0)\n",
    "    return prediction_mean, prediction_std\n",
    "\n",
    "# Make predictions with MC Dropout\n",
    "mean, std = mc_dropout_predictions(best_model, X)\n",
    "\n",
    "# Selecting new points with highest uncertainty\n",
    "num_new_points = 3  # Num of new points to sample\n",
    "new_points_indices = np.argsort(std)[-num_new_points:]\n",
    "new_points = X[new_points_indices]\n",
    "\n",
    "print(\"New points to sample:\")\n",
    "print(new_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Maximum Principal Elastic Strain (m/m)  Total Deformation (m)  \\\n",
      "0                            1.645777e-08           1.194049e-09   \n",
      "1                            1.462148e-08           1.061343e-09   \n",
      "2                            1.278519e-08           9.286820e-10   \n",
      "3                            1.096267e-08           7.960099e-10   \n",
      "4                            9.174577e-09           6.633378e-10   \n",
      "\n",
      "   Equivalent Elastic Strain (m/m) Dimensions (mm)  \n",
      "0                     2.171072e-08     400*410*100  \n",
      "1                     1.929829e-08     400*410*100  \n",
      "2                     1.688586e-08     400*410*100  \n",
      "3                     1.447343e-08     400*410*100  \n",
      "4                     1.206215e-08     400*410*100  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('../Data/drone_data.xlsx')\n",
    "\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
